<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      TRPO算法与代码解析 &middot; Namiyao Blog
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- Mathjax -->
  <script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>
  <script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p></p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="/">Home</a>
<!--
    

    
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/about/">About</a>
        
      
    
      
    

    <a class="sidebar-nav-item" href="/archive/v1.0.0.zip">Download</a>
    <a class="sidebar-nav-item" href="">GitHub project</a>
    <span class="sidebar-nav-item">Currently v1.0.0</span>

-->
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2016. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">Namiyao Blog</a>
            <small></small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">TRPO算法与代码解析</h1>
  <span class="post-date">03 Nov 2016</span>
  <h1 id="introduction">Introduction</h1>
<p><a href="http://karpathy.github.io/2016/05/31/rl/">Andrej Karpathy</a>指出，Policy Gradients (PG)是default的Reinforcement Learning (RL)算法。文章<a href="https://arxiv.org/abs/1604.06778">Benchmarking Deep Reinforcement Learning for Continuous Control</a>指出，Truncated Natural Policy Gradient (TNPG)算法，Trust Region Policy Optimization (TRPO)算法，Deep Deterministic Policy Gradient (DDPG)算法取得了最好的实验结果。除此之外，文章中未提到的
<a href="https://arxiv.org/abs/1602.01783">Asynchronous Advantage Actor-Critic (A3C)</a>算法的表现也超过了DQN。以上四种算法均属于PG。</p>

<p>DDPG算法与代码解析参考<a href="http://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html">Deep Deterministic Policy Gradients in TensorFlow</a>。</p>

<p>TNPG与TRPO算法的区别仅在于TRPO用了Backtracking line search来确定步长，从而使目标函数有足够的优化，而TNPG并没有使用Backtracking line search。本文对TRPO算法与代码进行解析，TNPG只需要去掉Backtracking line search这一步即可。</p>

<p>关于TRPO算法的文章主要有两篇。文章<a href="https://arxiv.org/abs/1502.05477">Trust Region Policy Optimization</a>提出了TRPO算法。文章<a href="https://arxiv.org/abs/1506.02438">High-Dimensional Continuous Control using Generalized Advantage Estimation</a>使用Generalized Advantage Estimator (GAE)改进了TRPO算法。</p>

<p>本文使用Wojciech Zaremba的基于Tensorflow的<a href="https://github.com/wojzaremba/trpo">代码</a>。</p>

<h1 id="start-with-some-theory">Start with some Theory</h1>

<h2 id="policy-gradients">Policy Gradients</h2>
<p>我们用函数来近似策略函数，记作 $\pi_{\theta}(a|s)$。目标函数为expected discounted reward，</p>

<p><script type="math/tex">J(\theta)=E[\sum_{t=0}^{\infty}\gamma^{t}r_{t}]</script>
要最大化目标函数，最直接的想法就是使用梯度下降算法</p>

<h2 id="advantage-function-estimation">Advantage Function Estimation</h2>

<h1 id="show-me-the-code">Show me the Code!</h1>


</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2014/01/02/introducing-lanyon/">
            Introducing Lanyon
            <small>02 Jan 2014</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2014/01/01/example-content/">
            Example content
            <small>01 Jan 2014</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2013/12/31/whats-jekyll/">
            What's Jekyll?
            <small>31 Dec 2013</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if(!checkbox.checked ||
             sidebar.contains(target) ||
             (target === checkbox || target === toggle)) return;

          checkbox.checked = false;
        }, false);
      })(document);
    </script>
  </body>
</html>
