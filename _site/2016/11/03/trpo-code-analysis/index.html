<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      TRPO算法与代码解析 &middot; Namiyao Blog
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- Mathjax -->
  <script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>
  <script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>This Blog is about Deep Learning and Reinforcement Learning.</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="/">Home</a>
<!--
    

    
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/about/">About</a>
        
      
    
      
    

    <a class="sidebar-nav-item" href="/archive/v1.0.0.zip">Download</a>
    <a class="sidebar-nav-item" href="">GitHub project</a>
    <span class="sidebar-nav-item">Currently v1.0.0</span>

-->
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2016. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">Namiyao Blog</a>
            <small></small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">TRPO算法与代码解析</h1>
  <span class="post-date">03 Nov 2016</span>
  <h1 id="introduction">Introduction</h1>
<p><a href="http://karpathy.github.io/2016/05/31/rl/">Andrej Karpathy</a>指出，Policy Gradients (PG)是default的Reinforcement Learning (RL)算法。文章<a href="https://arxiv.org/abs/1604.06778">Benchmarking Deep Reinforcement Learning for Continuous Control</a>指出，Truncated Natural Policy Gradient (TNPG)算法，Trust Region Policy Optimization (TRPO)算法，Deep Deterministic Policy Gradient (DDPG)算法取得了最好的实验结果。除此之外，文章中未提到的
<a href="https://arxiv.org/abs/1602.01783">Asynchronous Advantage Actor-Critic (A3C)</a>算法的表现也超过了DQN。以上四种算法均属于PG。</p>

<p>DDPG算法与代码解析参考<a href="http://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html">Deep Deterministic Policy Gradients in TensorFlow</a>。</p>

<p>TNPG与TRPO算法的区别仅在于TRPO用了Backtracking line search来确定步长，从而使目标函数有足够的优化，而TNPG并没有使用Backtracking line search。本文对TRPO算法与代码进行解析，TNPG只需要去掉Backtracking line search这一步即可。</p>

<p>关于TRPO算法的文章主要有两篇。文章<a href="https://arxiv.org/abs/1502.05477">Trust Region Policy Optimization</a>提出了TRPO算法。文章<a href="https://arxiv.org/abs/1506.02438">High-Dimensional Continuous Control using Generalized Advantage Estimation</a>使用Generalized Advantage Estimator (GAE)改进了TRPO算法。</p>

<p>本文使用Wojciech Zaremba的基于Tensorflow的<a href="https://github.com/wojzaremba/trpo">代码</a>。</p>

<h1 id="start-with-some-theory">Start with some Theory</h1>

<h2 id="policy-gradients">Policy Gradients</h2>
<p>我们用函数来近似policy，记作 $\pi_{\theta}(a|s)$。目标函数为expected discounted reward，</p>

<script type="math/tex; mode=display">J(\theta)=E[\sum_{t=0}^{\infty}\gamma^{t}r_{t}]</script>

<p>要最大化目标函数 $J(\theta)$，最直接的想法就是使用梯度下降算法，需要计算 $\nabla_{\theta}J(\theta)$。这个看起来超难计算的！幸好我们有 <em>Policy Gradients Theorem</em>，</p>

<script type="math/tex; mode=display">\nabla_{\theta}J(\theta)=E_{\pi_{\theta}}[\nabla_{\theta}log\pi(a|s)Q^{\pi_{\theta}}(s,a)]</script>

<p>用advantage function代替state-action value function，容易证明上式仍然成立</p>

<script type="math/tex; mode=display">\nabla_{\theta}J(\theta)=E_{\pi_{\theta}}[\nabla_{\theta}log\pi_{\theta}(a|s)A^{\pi_{\theta}}(s,a)]</script>

<p>其中 $A^{\pi_{\theta}}(s,a)=Q^{\pi_{\theta}}(s,a)-V^{\pi_{\theta}}(s)$。</p>

<p>现在 $\nabla_{\theta}J(\theta)$ 好算多了！我们只需要知道 $A^{\pi_{\theta}}(s,a)$ 就行了！记 $\hat A_{t}$ 为 $A^{\pi_{\theta}}(s_{t},a_{t})$ 的估计，则policy gradient estimator为</p>

<script type="math/tex; mode=display">\widehat{\nabla_{\theta}J(\theta)}=\frac{1}{N}\sum_{n=1}^{N}\sum_{t=0}^{\infty}\hat A_{t}^{n}\nabla_{\theta}log\pi_\theta(a_{t}^{n}|s_{t}^{n})</script>

<p>一个方法是用REINFORCE算法通过a batch of trajectories直接估计$A^{\pi_{\theta}}(s,a)$。下一节我们用函数近似方法来估计 $A^{\pi_{\theta}}(s,a)$。</p>

<h2 id="advantage-function-estimation">Advantage Function Estimation</h2>
<p>类似于 $TD(\lambda)$ 方法，以下都是 $A^{\pi_{\theta}}(s_{t},a_{t})$ 的估计</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\hat A_{t}^{(1)}&=r_{t}+\gamma V_{\phi}(s_{t+1})-V_{\phi}(s_{t})\\
\hat A_{t}^{(2)}&=r_{t}+\gamma r_{t+1}+\gamma^{2} V_{\phi}(s_{t+2})-V_{\phi}(s_{t})\\
\hat A_{t}^{(3)}&=r_{t}+\gamma r_{t+1}+\gamma^{2} r_{t+2}+\gamma^{3} V_{\phi}(s_{t+3})-V_{\phi}(s_{t})\\
...\\
\hat A_{t}^{(k)}&=r_{t}+\gamma r_{t+1}+...+\gamma^{k-1} r_{t+k-1}+\gamma^{k} V_{\phi}(s_{t+k})-V_{\phi}(s_{t})\\
...\\
\hat A_{t}^{(\infty)}&=\sum_{l=0}^{\infty}\gamma^{l}r_{t+l}-V_{\phi}(s_{t}) \tag{1}\label{A_inf}
\end{align} %]]></script>

<p>其中 $V_{\phi}(s_{t})$ 是value function $V^{\pi_{\theta}}(s_{t})$ 的函数近似。随着k的增加，估计的variance增加，bias减小。</p>

<p>Generalized Advantage Estimator (GAE)是使用以上估计的exponentially-weighted average，记作 $\hat A_{t}^{GAE(\gamma,\lambda)}$，</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\hat A_{t}^{GAE(\gamma,\lambda)}&=(1-\lambda)(\hat A_{t}^{(1)}+\lambda \hat A_{t}^{(2)}+\lambda^3 \hat A_{t}^{(3)}+...)\\
&=\sum_{l=0}^{\infty}(\gamma\lambda)^{l}\delta_{t+l}^{V_{\phi}}
\end{align} %]]></script>

<p>其中 $\delta_{t+l}^{V_{\phi}}=r_{t}+\gamma V_{\phi}(s_{t+1})-V_{\phi}(s_{t})$，用第二个等式可以方便的计算 $\hat A_{t}^{GAE(\gamma,\lambda)}$。容易看出 $\lambda=0$ 时，$\hat A_{t}^{GAE(\gamma,\lambda)}=\hat A_{t}^{(1)}$；$\lambda=1$ 时，$\hat A_{t}^{GAE(\gamma,\lambda)}=\hat A_{t}^{(\infty)}$。GAE通过exponentially-weighted average进行了bias-variance tradeoff，$\lambda$越大，后面的估计的权重越大，bias越小，variance越大。</p>

<p>以上我们分别用函数近似了policy和value function，这种方法叫做Actor-Critic算法。我们通过policy gradient estimator $\widehat{\nabla_{\theta}J(\theta)}$ 来更新 $\pi_{\theta}(a|s)$ 的参数 $\theta$。那么如何更新 $V_{\phi}(s)$ 的参数 $\phi$？最直观的想法是最小化L2损失</p>

<script type="math/tex; mode=display">\min_{\phi}\sum_{n=1}^{N}\sum_{t=0}^{\infty}(\hat V(s_{t})-V_{\phi}(s_{t}))^2</script>

<p>其中 $\hat V(s_{t})=\sum_{l=0}^{\infty}\gamma^{l}r_{t+l}$。可以通过梯度下降算法或者trust region算法来更新 $\phi$。</p>

<p><a href="https://arxiv.org/abs/1602.01783">A3C</a>算法使用 $\hat A_{t}^{(\infty)}$ 计算 policy gradient，然后用梯度下降算法来更新policy参数；并使用梯度下降算法来更新value function参数。</p>

<p><a href="https://arxiv.org/abs/1506.02438">TRPO+GAE</a>算法使用 $\hat A_{t}^{GAE(\gamma,\lambda)}$ 计算 policy gradient，然后用TRPO算法来更新policy参数；并使用trust region算法来更新value function参数。</p>

<h1 id="show-me-the-code">Show me the Code!</h1>

<p>下一节的代码解析使用</p>

<p>$\eqref{A_inf}$</p>


</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2014/01/02/introducing-lanyon/">
            Introducing Lanyon
            <small>02 Jan 2014</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if(!checkbox.checked ||
             sidebar.contains(target) ||
             (target === checkbox || target === toggle)) return;

          checkbox.checked = false;
        }, false);
      })(document);
    </script>
  </body>
</html>
