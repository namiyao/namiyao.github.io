<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Namiyao Blog</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000/"/>
 <updated>2016-11-06T15:02:33+08:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name>Mark Otto</name>
   <email>markdotto@gmail.com</email>
 </author>

 
 <entry>
   <title>TRPO算法与代码解析</title>
   <link href="http://localhost:4000/2016/11/03/trpo-code-analysis/"/>
   <updated>2016-11-03T00:00:00+08:00</updated>
   <id>http://localhost:4000/2016/11/03/trpo-code-analysis</id>
   <content type="html">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;http://karpathy.github.io/2016/05/31/rl/&quot;&gt;Andrej Karpathy&lt;/a&gt;指出，Policy Gradients (PG)是default的Reinforcement Learning (RL)算法。文章&lt;a href=&quot;https://arxiv.org/abs/1604.06778&quot;&gt;Benchmarking Deep Reinforcement Learning for Continuous Control&lt;/a&gt;指出，Truncated Natural Policy Gradient (TNPG)算法，Trust Region Policy Optimization (TRPO)算法，Deep Deterministic Policy Gradient (DDPG)算法取得了最好的实验结果。除此之外，文章中未提到的
&lt;a href=&quot;https://arxiv.org/abs/1602.01783&quot;&gt;Asynchronous Advantage Actor-Critic (A3C)&lt;/a&gt;算法的表现也超过了DQN。以上四种算法均属于PG。&lt;/p&gt;

&lt;p&gt;DDPG算法与代码解析参考&lt;a href=&quot;http://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html&quot;&gt;Deep Deterministic Policy Gradients in TensorFlow&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;TNPG与TRPO算法的区别仅在于TRPO用了Backtracking line search来确定步长，从而使目标函数有足够的优化，而TNPG并没有使用Backtracking line search。本文对TRPO算法与代码进行解析，TNPG只需要去掉Backtracking line search这一步即可。&lt;/p&gt;

&lt;p&gt;关于TRPO算法的文章主要有两篇。文章&lt;a href=&quot;https://arxiv.org/abs/1502.05477&quot;&gt;Trust Region Policy Optimization&lt;/a&gt;提出了TRPO算法。文章&lt;a href=&quot;https://arxiv.org/abs/1506.02438&quot;&gt;High-Dimensional Continuous Control using Generalized Advantage Estimation&lt;/a&gt;使用Generalized Advantage Estimator (GAE)改进了TRPO算法。&lt;/p&gt;

&lt;p&gt;本文使用Wojciech Zaremba的基于Tensorflow的&lt;a href=&quot;https://github.com/wojzaremba/trpo&quot;&gt;代码&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&quot;start-with-some-theory&quot;&gt;Start with some Theory&lt;/h1&gt;

&lt;h2 id=&quot;policy-gradients&quot;&gt;Policy Gradients&lt;/h2&gt;
&lt;p&gt;我们用函数来近似policy，记作 $\pi_{\theta}(a|s)$。目标函数为expected discounted reward，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\theta)=E[\sum_{t=0}^{\infty}\gamma^{t}r_{t}]&lt;/script&gt;

&lt;p&gt;要最大化目标函数 $J(\theta)$，最直接的想法就是使用梯度下降算法，需要计算 $\nabla_{\theta}J(\theta)$。这个看起来超难计算的！幸好我们有 &lt;em&gt;Policy Gradients Theorem&lt;/em&gt;，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_{\theta}J(\theta)=E_{\pi_{\theta}}[\nabla_{\theta}log\pi(a|s)Q^{\pi_{\theta}}(s,a)]&lt;/script&gt;

&lt;p&gt;用advantage function代替state-action value function，容易证明上式仍然成立&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_{\theta}J(\theta)=E_{\pi_{\theta}}[\nabla_{\theta}log\pi_{\theta}(a|s)A^{\pi_{\theta}}(s,a)]&lt;/script&gt;

&lt;p&gt;其中 $A^{\pi_{\theta}}(s,a)=Q^{\pi_{\theta}}(s,a)-V^{\pi_{\theta}}(s)$。&lt;/p&gt;

&lt;p&gt;现在 $\nabla_{\theta}J(\theta)$ 好算多了！我们只需要知道 $A^{\pi_{\theta}}(s,a)$ 就行了！记 $\hat A_{t}$ 为 $A^{\pi_{\theta}}(s_{t},a_{t})$ 的估计，则policy gradient estimator为&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\widehat{\nabla_{\theta}J(\theta)}=\frac{1}{N}\sum_{n=1}^{N}\sum_{t=0}^{\infty}\hat A_{t}^{n}\nabla_{\theta}log\pi_\theta(a_{t}^{n}|s_{t}^{n})&lt;/script&gt;

&lt;p&gt;一个方法是用REINFORCE算法通过a batch of trajectories直接估计$A^{\pi_{\theta}}(s,a)$。下一节我们用函数近似方法来估计 $A^{\pi_{\theta}}(s,a)$。&lt;/p&gt;

&lt;h2 id=&quot;advantage-function-estimation&quot;&gt;Advantage Function Estimation&lt;/h2&gt;
&lt;p&gt;类似于 $TD(\lambda)$ 方法，以下都是 $A^{\pi_{\theta}}(s_{t},a_{t})$ 的估计&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\hat A_{t}^{(1)}&amp;=r_{t}+\gamma V_{\phi}(s_{t+1})-V_{\phi}(s_{t})\\
\hat A_{t}^{(2)}&amp;=r_{t}+\gamma r_{t+1}+\gamma^{2} V_{\phi}(s_{t+2})-V_{\phi}(s_{t})\\
\hat A_{t}^{(3)}&amp;=r_{t}+\gamma r_{t+1}+\gamma^{2} r_{t+2}+\gamma^{3} V_{\phi}(s_{t+3})-V_{\phi}(s_{t})\\
...\\
\hat A_{t}^{(k)}&amp;=r_{t}+\gamma r_{t+1}+...+\gamma^{k-1} r_{t+k-1}+\gamma^{k} V_{\phi}(s_{t+k})-V_{\phi}(s_{t})\\
...\\
\hat A_{t}^{(\infty)}&amp;=\sum_{l=0}^{\infty}\gamma^{l}r_{t+l}-V_{\phi}(s_{t}) \tag{1}\label{A_inf}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;其中 $V_{\phi}(s_{t})$ 是value function $V^{\pi_{\theta}}(s_{t})$ 的函数近似。随着k的增加，估计的variance增加，bias减小。&lt;/p&gt;

&lt;p&gt;Generalized Advantage Estimator (GAE)是使用以上估计的exponentially-weighted average，记作 $\hat A_{t}^{GAE(\gamma,\lambda)}$，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\hat A_{t}^{GAE(\gamma,\lambda)}&amp;=(1-\lambda)(\hat A_{t}^{(1)}+\lambda \hat A_{t}^{(2)}+\lambda^3 \hat A_{t}^{(3)}+...)\\
&amp;=\sum_{l=0}^{\infty}(\gamma\lambda)^{l}\delta_{t+l}^{V_{\phi}}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;其中 $\delta_{t+l}^{V_{\phi}}=r_{t}+\gamma V_{\phi}(s_{t+1})-V_{\phi}(s_{t})$，用第二个等式可以方便的计算 $\hat A_{t}^{GAE(\gamma,\lambda)}$。容易看出 $\lambda=0$ 时，$\hat A_{t}^{GAE(\gamma,\lambda)}=\hat A_{t}^{(1)}$；$\lambda=1$ 时，$\hat A_{t}^{GAE(\gamma,\lambda)}=\hat A_{t}^{(\infty)}$。GAE通过exponentially-weighted average进行了bias-variance tradeoff，$\lambda$越大，后面的估计的权重越大，bias越小，variance越大。&lt;/p&gt;

&lt;p&gt;以上我们分别用函数近似了policy和value function，这种方法叫做Actor-Critic算法。我们通过policy gradient estimator $\widehat{\nabla_{\theta}J(\theta)}$ 来更新 $\pi_{\theta}(a|s)$ 的参数 $\theta$。那么如何更新 $V_{\phi}(s)$ 的参数 $\phi$？最直观的想法是最小化L2损失&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_{\phi}\sum_{n=1}^{N}\sum_{t=0}^{\infty}(\hat V(s_{t})-V_{\phi}(s_{t}))^2&lt;/script&gt;

&lt;p&gt;其中 $\hat V(s_{t})=\sum_{l=0}^{\infty}\gamma^{l}r_{t+l}$。可以通过梯度下降算法或者trust region算法来更新 $\phi$。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1602.01783&quot;&gt;A3C&lt;/a&gt;算法使用 $\hat A_{t}^{(\infty)}$ 计算 policy gradient，然后用梯度下降算法来更新policy参数；并使用梯度下降算法来更新value function参数。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1506.02438&quot;&gt;TRPO+GAE&lt;/a&gt;算法使用 $\hat A_{t}^{GAE(\gamma,\lambda)}$ 计算 policy gradient，然后用TRPO算法来更新policy参数；并使用trust region算法来更新value function参数。&lt;/p&gt;

&lt;h1 id=&quot;show-me-the-code&quot;&gt;Show me the Code!&lt;/h1&gt;

&lt;p&gt;下一节的代码解析使用&lt;/p&gt;

&lt;p&gt;$\eqref{A_inf}$&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Introducing Lanyon</title>
   <link href="http://localhost:4000/2014/01/02/introducing-lanyon/"/>
   <updated>2014-01-02T00:00:00+08:00</updated>
   <id>http://localhost:4000/2014/01/02/introducing-lanyon</id>
   <content type="html">&lt;p&gt;Lanyon is an unassuming &lt;a href=&quot;http://jekyllrb.com&quot;&gt;Jekyll&lt;/a&gt; theme that places content first by tucking away navigation in a hidden drawer. It’s based on &lt;a href=&quot;http://getpoole.com&quot;&gt;Poole&lt;/a&gt;, the Jekyll butler.&lt;/p&gt;

&lt;h3 id=&quot;built-on-poole&quot;&gt;Built on Poole&lt;/h3&gt;

&lt;p&gt;Poole is the Jekyll Butler, serving as an upstanding and effective foundation for Jekyll themes by &lt;a href=&quot;https://twitter.com/mdo&quot;&gt;@mdo&lt;/a&gt;. Poole, and every theme built on it (like Lanyon here) includes the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Complete Jekyll setup included (layouts, config, &lt;a href=&quot;/404&quot;&gt;404&lt;/a&gt;, &lt;a href=&quot;/atom.xml&quot;&gt;RSS feed&lt;/a&gt;, posts, and &lt;a href=&quot;/about&quot;&gt;example page&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Mobile friendly design and development&lt;/li&gt;
  &lt;li&gt;Easily scalable text and component sizing with &lt;code class=&quot;highlighter-rouge&quot;&gt;rem&lt;/code&gt; units in the CSS&lt;/li&gt;
  &lt;li&gt;Support for a wide gamut of HTML elements&lt;/li&gt;
  &lt;li&gt;Related posts (time-based, because Jekyll) below each post&lt;/li&gt;
  &lt;li&gt;Syntax highlighting, courtesy Pygments (the Python-based code snippet highlighter)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;lanyon-features&quot;&gt;Lanyon features&lt;/h3&gt;

&lt;p&gt;In addition to the features of Poole, Lanyon adds the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Toggleable sliding sidebar (built with only CSS) via &lt;strong&gt;☰&lt;/strong&gt; link in top corner&lt;/li&gt;
  &lt;li&gt;Sidebar includes support for textual modules and a dynamically generated navigation with active link support&lt;/li&gt;
  &lt;li&gt;Two orientations for content and sidebar, default (left sidebar) and &lt;a href=&quot;https://github.com/poole/lanyon#reverse-layout&quot;&gt;reverse&lt;/a&gt; (right sidebar), available via &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;body&amp;gt;&lt;/code&gt; classes&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/poole/lanyon#themes&quot;&gt;Eight optional color schemes&lt;/a&gt;, available via &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;body&amp;gt;&lt;/code&gt; classes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/poole/lanyon#readme&quot;&gt;Head to the readme&lt;/a&gt; to learn more.&lt;/p&gt;

&lt;h3 id=&quot;browser-support&quot;&gt;Browser support&lt;/h3&gt;

&lt;p&gt;Lanyon is by preference a forward-thinking project. In addition to the latest versions of Chrome, Safari (mobile and desktop), and Firefox, it is only compatible with Internet Explorer 9 and above.&lt;/p&gt;

&lt;h3 id=&quot;download&quot;&gt;Download&lt;/h3&gt;

&lt;p&gt;Lanyon is developed on and hosted with GitHub. Head to the &lt;a href=&quot;https://github.com/poole/lanyon&quot;&gt;GitHub repository&lt;/a&gt; for downloads, bug reports, and features requests.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
</content>
 </entry>
 

</feed>
